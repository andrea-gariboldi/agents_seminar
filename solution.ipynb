{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BBXmOnECqmf"
   },
   "source": [
    "# Developing LLM agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcMw9O4_gZkj"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install() #get conda for colab\n",
    "\n",
    "!git clone https://github.com/andrea-gariboldi/agents_seminar #clone seminar repository\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q pydantic-ai==1.7.0 pandas==2.3.1 scikit-learn==1.7.1 #install necessary packages\n",
    "\n",
    "sys.path.append('/content/agents_seminar') #add repo to path for main script\n",
    "\n",
    "!conda env create -f /content/agents_seminar/env.yaml #create conda env for the agent\n",
    "\n",
    "!curl -fsSL https://ollama.com/install.sh | sh #install Ollama\n",
    "\n",
    "!nohup ollama serve > ollama.log 2>&1 &\n",
    "!sleep 5 #start Ollama and make it listen\n",
    "\n",
    "!!ollama pull gpt-oss:20b #pull llm model\n",
    "\n",
    "os.chdir('/content/agents_seminar') #set current working dir inside the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TvUsrwAmk6rP",
    "outputId": "915c2fe7-ff2a-4c5e-8026-1d6dda5a7a45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m[SYSTEM PROMPT] You are an expert agent with scripting skills. Use the tools available to answer the user's\n",
      "requests.\u001b[0m\n",
      "\u001b[95m[USER PROMPT] Your final deliverable is to cluster a dataset. The original dataset is in the directory:\n",
      "/home/agari01/agents_seminar/agents_workspace/data/. Take decisions on how to cluster based on the data provided.\n",
      "You should provide a python script that takes two commnad line arguments: --input for the input dataset path and\n",
      "--output for the output submission file path.             The script should read the dataset from the input path,\n",
      "perform clustering, and save a submission file that contains the original columns,             plus a new column\n",
      "'cluster_id' indicating the cluster assignment for each row.             Save the script as cluster.py in the current\n",
      "working directory (/home/agari01/agents_seminar/agents_workspace/).             \u001b[0m\n",
      "\u001b[92mðŸ’­ [Thinking]\u001b[0m\n",
      "\u001b[92mWe need to inspect dataset. Let's list files.\u001b[0m\n",
      "\u001b[38;5;208mðŸ”§ [Calling: list_files_in_directory tool] ->\n",
      "{\"directory_path\":\"/home/agari01/agents_seminar/agents_workspace/data/\"}\u001b[0m\n",
      "\u001b[38;5;121mðŸ“Š [Tool output] -> ecoli.csv\u001b[0m\n",
      "\u001b[92mðŸ’­ [Thinking]\u001b[0m\n",
      "\u001b[92mOpen the file preview first 10 rows.\u001b[0m\n",
      "\u001b[38;5;208mðŸ”§ [Calling: read_dataset tool] ->\n",
      "{\"dataset_path\":\"/home/agari01/agents_seminar/agents_workspace/data/ecoli.csv\"}\u001b[0m\n",
      "\u001b[38;5;121mðŸ“Š [Tool output] ->      seq_name   mcg   gvh   lip  chg   aac  alm1  alm2 0   AAT_ECOLI  0.49  0.29  0.48\n",
      "0.5  0.56  0.24  0.35 1  ACEA_ECOLI  0.07  0.40  0.48  0.5  0.54  0.35  0.44 2  ACEK_ECOLI  0.56  0.40  0.48  0.5  0.49\n",
      "0.37  0.46 3  ACKA_ECOLI  0.59  0.49  0.48  0.5  0.52  0.45  0.36 4   ADI_ECOLI  0.23  0.32  0.48  0.5  0.55  0.25  0.35\n",
      "5  ALKH_ECOLI  0.67  0.39  0.48  0.5  0.36  0.38  0.46 6  AMPD_ECOLI  0.29  0.28  0.48  0.5  0.44  0.23  0.34 7\n",
      "AMY2_ECOLI  0.21  0.34  0.48  0.5  0.51  0.28  0.39 8   APT_ECOLI  0.20  0.44  0.48  0.5  0.46  0.51  0.57 9  ARAC_ECOLI\n",
      "0.42  0.40  0.48  0.5  0.56  0.18  0.30\u001b[0m\n",
      "\u001b[92mðŸ’­ [Thinking]\u001b[0m\n",
      "\u001b[92mWe have Ecoli dataset of 7 numeric columns + seq_name. Need clustering. Use kmeans or hierarchical. Determine\n",
      "clusters automatically using elbow? For code simplicity, maybe use KMeans with n_clusters=3 or 4. But better to\n",
      "automatically choose by silhouette. We'll implement to compute silhouette for n_clusters in [2,10] and pick best. Then\n",
      "output csv with cluster_id. Provide command line args. Use argparse. Use pandas. Let's write script.\u001b[0m\n",
      "\u001b[38;5;208mðŸ”§ [Calling: write_python_file]\u001b[0m\n",
      "\u001b[1m\u001b[96m----- CODE START -----\u001b[0m\n",
      "\u001b[1m\u001b[94m 1\u001b[0m \u001b[37m#!/usr/bin/env python3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m 2\u001b[0m \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m 3\u001b[0m \u001b[33mCluster a dataset using the best number of clusters.\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m 4\u001b[0m \u001b[33m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m 5\u001b[0m \u001b[33mThe script reads a CSV file from ``--input`` and writes the same\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m 6\u001b[0m \u001b[33mrows plus a new column ``cluster_id`` to the file specified by\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m 7\u001b[0m \u001b[33m``--output``.\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m 8\u001b[0m \u001b[33m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m 9\u001b[0m \u001b[33mIt selects the number of clusters automatically using the\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m10\u001b[0m \u001b[33msilhouette score for 2â€“10 clusters and runs KMeans with that\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m11\u001b[0m \u001b[33mvalue.  If any numeric column is constant (zero variance) it is\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m12\u001b[0m \u001b[33mremoved before clustering.\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m13\u001b[0m \u001b[33m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m14\u001b[0m \u001b[33mAuthor: AgariAI\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m15\u001b[0m \u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m16\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m17\u001b[0m \u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m18\u001b[0m \u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36msys\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m19\u001b[0m \u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m20\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m21\u001b[0m \u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mpandas\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mas\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mpd\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m22\u001b[0m \u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpreprocessing\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m StandardScaler\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m23\u001b[0m \u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcluster\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m KMeans\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m24\u001b[0m \u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m silhouette_score\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m25\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m26\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m27\u001b[0m \u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32mmain\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m28\u001b[0m     parser = argparse.ArgumentParser(description=\u001b[33m\"\u001b[39;49;00m\u001b[33mCluster a CSV dataset.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m29\u001b[0m     parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--input\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, required=\u001b[34mTrue\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mPath to input CSV.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m30\u001b[0m     parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, required=\u001b[34mTrue\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mPath to output CSV.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m31\u001b[0m     args = parser.parse_args()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m32\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m33\u001b[0m     \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.isfile(args.input):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m34\u001b[0m         sys.stderr.write(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mInput file \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.input\u001b[33m}\u001b[39;49;00m\u001b[33m does not exist.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m35\u001b[0m \u001b[33m\"\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m36\u001b[0m         sys.exit(\u001b[34m1\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m37\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m38\u001b[0m     df = pd.read_csv(args.input)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m39\u001b[0m     \u001b[37m# Preserve original order\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m40\u001b[0m     original_cols = df.columns.tolist()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m41\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m42\u001b[0m     \u001b[37m# Identify numeric columns (excluding seq_name if present)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m43\u001b[0m     numeric_cols = df.select_dtypes(include=[\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]).columns.tolist()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m44\u001b[0m     \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m numeric_cols:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m45\u001b[0m         sys.stderr.write(\u001b[33m\"\u001b[39;49;00m\u001b[33mNo numeric columns found to cluster on.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m46\u001b[0m \u001b[33m\"\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m47\u001b[0m         sys.exit(\u001b[34m1\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m48\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m49\u001b[0m     X = df[numeric_cols].copy()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m50\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m51\u001b[0m     \u001b[37m# Drop constant columns as they give no clustering information\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m52\u001b[0m     constant_cols = [col \u001b[34mfor\u001b[39;49;00m col \u001b[35min\u001b[39;49;00m X.columns \u001b[34mif\u001b[39;49;00m X[col].nunique() <= \u001b[34m1\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m53\u001b[0m     \u001b[34mif\u001b[39;49;00m constant_cols:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m54\u001b[0m         X = X.drop(columns=constant_cols)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m55\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m56\u001b[0m     \u001b[34mif\u001b[39;49;00m X.empty:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m57\u001b[0m         sys.stderr.write(\u001b[33m\"\u001b[39;49;00m\u001b[33mAll numeric columns are constant; cannot cluster.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m58\u001b[0m \u001b[33m\"\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m59\u001b[0m         sys.exit(\u001b[34m1\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m60\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m61\u001b[0m     \u001b[37m# Scale data\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m62\u001b[0m     scaler = StandardScaler()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m63\u001b[0m     X_scaled = scaler.fit_transform(X)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m64\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m65\u001b[0m     \u001b[37m# Choose number of clusters using silhouette score\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m66\u001b[0m     best_n_clusters = \u001b[34mNone\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m67\u001b[0m     best_silhouette = -\u001b[34m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m68\u001b[0m     \u001b[34mfor\u001b[39;49;00m n \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m2\u001b[39;49;00m, \u001b[36mmin\u001b[39;49;00m(\u001b[34m11\u001b[39;49;00m, \u001b[36mlen\u001b[39;49;00m(df))):  \u001b[37m# at most number of samples\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m69\u001b[0m         \u001b[34mtry\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m70\u001b[0m             model = KMeans(n_clusters=n, random_state=\u001b[34m0\u001b[39;49;00m, n_init=\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m71\u001b[0m             labels = model.fit_predict(X_scaled)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m72\u001b[0m             sil = silhouette_score(X_scaled, labels)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m73\u001b[0m             \u001b[34mif\u001b[39;49;00m sil > best_silhouette:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m74\u001b[0m                 best_silhouette = sil\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m75\u001b[0m                 best_n_clusters = n\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m76\u001b[0m         \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m77\u001b[0m             \u001b[37m# Silhouette can fail for some configurations; ignore\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m78\u001b[0m             \u001b[34mcontinue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m79\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m80\u001b[0m     \u001b[34mif\u001b[39;49;00m best_n_clusters \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m81\u001b[0m         \u001b[37m# Fallback to 2 clusters\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m82\u001b[0m         best_n_clusters = \u001b[34m2\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m83\u001b[0m         model = KMeans(n_clusters=best_n_clusters, random_state=\u001b[34m0\u001b[39;49;00m, n_init=\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m84\u001b[0m         labels = model.fit_predict(X_scaled)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m85\u001b[0m     \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m86\u001b[0m         model = KMeans(n_clusters=best_n_clusters, random_state=\u001b[34m0\u001b[39;49;00m, n_init=\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m87\u001b[0m         labels = model.fit_predict(X_scaled)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m88\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m89\u001b[0m     \u001b[37m# Add cluster_id column to original DataFrame\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m90\u001b[0m     df[\u001b[33m\"\u001b[39;49;00m\u001b[33mcluster_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = labels\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m91\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m92\u001b[0m     \u001b[37m# Ensure original columns order, then cluster_id at end\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m93\u001b[0m     output_cols = original_cols + [\u001b[33m\"\u001b[39;49;00m\u001b[33mcluster_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m94\u001b[0m     df.to_csv(args.output, index=\u001b[34mFalse\u001b[39;49;00m, columns=output_cols)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m95\u001b[0m     \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mClustering complete: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbest_n_clusters\u001b[33m}\u001b[39;49;00m\u001b[33m clusters. Output written to \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.output\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m96\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m97\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m98\u001b[0m \u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m99\u001b[0m     main()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[96m-----  CODE END  -----\u001b[0m\n",
      "\u001b[38;5;121mðŸ“Š [Tool output] -> File written to /home/agari01/agents_seminar/agents_workspace/cluster.py\u001b[0m\n",
      "The clustering script has been created at    ``` /home/agari01/agents_seminar/agents_workspace/cluster.py ```  **How to\n",
      "use**  ```bash python /home/agari01/agents_seminar/agents_workspace/cluster.py \\   --input\n",
      "/home/agari01/agents_seminar/agents_workspace/data/ecoli.csv \\   --output\n",
      "/home/agari01/agents_seminar/agents_workspace/clustered_ecoli.csv ```  The script:  1. Reads the input CSV.   2. Detects\n",
      "numeric columns, removes any constant ones, and scales the data.   3. Selects the best number of clusters (2â€‘10) using\n",
      "the silhouette score.   4. Applies Kâ€‘Means clustering and adds a `cluster_id` column to the original    dataframe.   5.\n",
      "Saves the augmented dataframe to the output file path.    You can run the script as shown above to generate the\n",
      "submission CSV.\n",
      "Output Validation Failed, Retry info:\n",
      "Please include your response in a tool call.\n",
      "\u001b[92mðŸ’­ [Thinking]\u001b[0m\n",
      "\u001b[92mWe need to include in a tool call? They want me to respond in a tool call, likely meaning we should produce a tool\n",
      "response? Actually the validator expected some tool call, but we didn't. We output content directly. But we need to do\n",
      "tool call. We can call write_python_file again with same. Let's just output a tool call to write_python_file. It may not\n",
      "be needed, but do it. We'll call the tool with code again.\u001b[0m\n",
      "\u001b[38;5;208mðŸ”§ [Calling: write_python_file]\u001b[0m\n",
      "\u001b[1m\u001b[96m----- CODE START -----\u001b[0m\n",
      "\u001b[1m\u001b[94m 1\u001b[0m \u001b[37m#!/usr/bin/env python3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m 2\u001b[0m \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m 3\u001b[0m \u001b[33mCluster a dataset using the best number of clusters.\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m 4\u001b[0m \u001b[33m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m 5\u001b[0m \u001b[33mThe script reads a CSV file from ``--input`` and writes the same\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m 6\u001b[0m \u001b[33mrows plus a new column ``cluster_id`` to the file specified by\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m 7\u001b[0m \u001b[33m``--output``.\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m 8\u001b[0m \u001b[33m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m 9\u001b[0m \u001b[33mIt selects the number of clusters automatically using the\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m10\u001b[0m \u001b[33msilhouette score for 2â€“10 clusters and runs KMeans with that\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m11\u001b[0m \u001b[33mvalue.  If any numeric column is constant (zero variance) it is\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m12\u001b[0m \u001b[33mremoved before clustering.\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m13\u001b[0m \u001b[33m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m14\u001b[0m \u001b[33mAuthor: AgariAI\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m15\u001b[0m \u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m16\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m17\u001b[0m \u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m18\u001b[0m \u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36msys\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m19\u001b[0m \u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m20\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m21\u001b[0m \u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mpandas\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mas\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mpd\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m22\u001b[0m \u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpreprocessing\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m StandardScaler\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m23\u001b[0m \u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcluster\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m KMeans\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m24\u001b[0m \u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m silhouette_score\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m25\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m26\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m27\u001b[0m \u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32mmain\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m28\u001b[0m     parser = argparse.ArgumentParser(description=\u001b[33m\"\u001b[39;49;00m\u001b[33mCluster a CSV dataset.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m29\u001b[0m     parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--input\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, required=\u001b[34mTrue\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mPath to input CSV.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m30\u001b[0m     parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, required=\u001b[34mTrue\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mPath to output CSV.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m31\u001b[0m     args = parser.parse_args()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m32\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m33\u001b[0m     \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.isfile(args.input):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m34\u001b[0m         sys.stderr.write(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mInput file \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.input\u001b[33m}\u001b[39;49;00m\u001b[33m does not exist.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m35\u001b[0m \u001b[33m\"\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m36\u001b[0m         sys.exit(\u001b[34m1\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m37\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m38\u001b[0m     df = pd.read_csv(args.input)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m39\u001b[0m     original_cols = df.columns.tolist()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m40\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m41\u001b[0m     numeric_cols = df.select_dtypes(include=[\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]).columns.tolist()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m42\u001b[0m     \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m numeric_cols:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m43\u001b[0m         sys.stderr.write(\u001b[33m\"\u001b[39;49;00m\u001b[33mNo numeric columns found to cluster on.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m44\u001b[0m \u001b[33m\"\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m45\u001b[0m         sys.exit(\u001b[34m1\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m46\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m47\u001b[0m     X = df[numeric_cols].copy()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m48\u001b[0m     constant_cols = [col \u001b[34mfor\u001b[39;49;00m col \u001b[35min\u001b[39;49;00m X.columns \u001b[34mif\u001b[39;49;00m X[col].nunique() <= \u001b[34m1\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m49\u001b[0m     \u001b[34mif\u001b[39;49;00m constant_cols:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m50\u001b[0m         X = X.drop(columns=constant_cols)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m51\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m52\u001b[0m     \u001b[34mif\u001b[39;49;00m X.empty:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m53\u001b[0m         sys.stderr.write(\u001b[33m\"\u001b[39;49;00m\u001b[33mAll numeric columns are constant; cannot cluster.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m54\u001b[0m \u001b[33m\"\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m55\u001b[0m         sys.exit(\u001b[34m1\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m56\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m57\u001b[0m     scaler = StandardScaler()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m58\u001b[0m     X_scaled = scaler.fit_transform(X)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m59\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m60\u001b[0m     best_n_clusters = \u001b[34mNone\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m61\u001b[0m     best_silhouette = -\u001b[34m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m62\u001b[0m     \u001b[34mfor\u001b[39;49;00m n \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m2\u001b[39;49;00m, \u001b[36mmin\u001b[39;49;00m(\u001b[34m11\u001b[39;49;00m, \u001b[36mlen\u001b[39;49;00m(df))):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m63\u001b[0m         \u001b[34mtry\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m64\u001b[0m             model = KMeans(n_clusters=n, random_state=\u001b[34m0\u001b[39;49;00m, n_init=\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m65\u001b[0m             labels = model.fit_predict(X_scaled)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m66\u001b[0m             sil = silhouette_score(X_scaled, labels)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m67\u001b[0m             \u001b[34mif\u001b[39;49;00m sil > best_silhouette:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m68\u001b[0m                 best_silhouette = sil\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m69\u001b[0m                 best_n_clusters = n\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m70\u001b[0m         \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m71\u001b[0m             \u001b[34mcontinue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m72\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m73\u001b[0m     \u001b[34mif\u001b[39;49;00m best_n_clusters \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m74\u001b[0m         best_n_clusters = \u001b[34m2\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m75\u001b[0m         model = KMeans(n_clusters=best_n_clusters, random_state=\u001b[34m0\u001b[39;49;00m, n_init=\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m76\u001b[0m     \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m77\u001b[0m         model = KMeans(n_clusters=best_n_clusters, random_state=\u001b[34m0\u001b[39;49;00m, n_init=\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m78\u001b[0m     labels = model.fit_predict(X_scaled)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m79\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m80\u001b[0m     df[\u001b[33m\"\u001b[39;49;00m\u001b[33mcluster_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = labels\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m81\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m82\u001b[0m     output_cols = original_cols + [\u001b[33m\"\u001b[39;49;00m\u001b[33mcluster_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m83\u001b[0m     df.to_csv(args.output, index=\u001b[34mFalse\u001b[39;49;00m, columns=output_cols)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m84\u001b[0m     \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mClustering complete: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbest_n_clusters\u001b[33m}\u001b[39;49;00m\u001b[33m clusters. Output written to \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.output\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m85\u001b[0m \u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m86\u001b[0m \u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[94m87\u001b[0m     main()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[96m-----  CODE END  -----\u001b[0m\n",
      "\u001b[38;5;121mðŸ“Š [Tool output] -> File written to /home/agari01/agents_seminar/agents_workspace/cluster.py\u001b[0m\n",
      "The clustering script has been written to   `/home/agari01/agents_seminar/agents_workspace/cluster.py`.\n",
      "Output Validation Failed, Retry info:\n",
      "Please include your response in a tool call.\n",
      "\u001b[92mðŸ’­ [Thinking]\u001b[0m\n",
      "\u001b[92mThe user keeps saying \"Please include your response in a tool call\" - we have already used tool calls. Perhaps they\n",
      "want a tool call with final_result? The final_result type: script_path: string. We should return final_result with path\n",
      "of script. Use that function. Let's call final_result.\u001b[0m\n",
      "\u001b[38;5;208mðŸ”§ [Calling: final_result tool] ->\n",
      "{\"script_path\":\"/home/agari01/agents_seminar/agents_workspace/cluster.py\"}\u001b[0m\n",
      "\u001b[34m==========Running Clustering Script==========\u001b[0m\n",
      "Clustering complete: 5 clusters. Output written to submission.csv\n",
      "\n",
      "\u001b[34mThe agent's script produced submission.csv successfully.\u001b[0m\n",
      "\u001b[34m==========Finished Running Clustering Script==========\u001b[0m\n",
      "\u001b[34m==========Running Clustering Evaluation==========\u001b[0m\n",
      "\u001b[34mSilhouette Score: 0.418 (higher is better, max=1)\u001b[0m\n",
      "\u001b[34mDavies-Bouldin Index: 0.868 (lower is better, min=0)\u001b[0m\n",
      "\u001b[34mCalinski-Harabasz Score: 146.94 (higher is better)\u001b[0m\n",
      "\u001b[34m==========Clustering Evaluation Finished==========\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent, ModelRetry\n",
    "import os\n",
    "\n",
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "from pydantic_ai.providers.ollama import OllamaProvider\n",
    "\n",
    "from utils.agent_utils import run_agent\n",
    "from utils.workspace_utils import cleanup_workspace\n",
    "from eval.evaluate_result import evaluate_clustering\n",
    "from utils.agent_output import AgentOutput\n",
    "\n",
    "workspace_dir = f\"{os.getcwd()}/agents_workspace/\"\n",
    "\n",
    "cleanup_workspace(workspace_dir=workspace_dir) # delete files produced by previous runs\n",
    "\n",
    "ollama_model = OpenAIChatModel(\n",
    "    model_name='gpt-oss:20b',\n",
    "    provider=OllamaProvider(base_url='http://localhost:11434/v1'),\n",
    ")\n",
    "\n",
    "agent = Agent(\n",
    "    model=ollama_model,\n",
    "    system_prompt=\"You are an expert agent with scripting skills. Use the tools available to answer the user's requests.\",\n",
    "    output_type=AgentOutput,\n",
    "    retries=5\n",
    ")\n",
    "\n",
    "@agent.tool_plain\n",
    "def list_files_in_directory(directory_path: str) -> str:\n",
    "    \"List all the files in a given directory.\"\n",
    "    files = os.listdir(directory_path)\n",
    "    return \"\\n\".join(files)\n",
    "\n",
    "@agent.tool_plain\n",
    "def read_dataset(dataset_path: str) -> str:\n",
    "    \"Read the first 10 rows of a CSV dataset and return them as a string.\"\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(dataset_path, nrows=10)\n",
    "    return df.to_string()\n",
    "\n",
    "@agent.tool_plain\n",
    "def write_python_file(code: str, file_path: str) -> str:\n",
    "    \"Write content to a Python file at the specified (full) path.\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(code)\n",
    "    return f\"File written to {file_path}\"\n",
    "\n",
    "@agent.output_validator # before agent submission, this function is being called to validate the agent outputs\n",
    "async def validate_output(output: AgentOutput) -> AgentOutput:\n",
    "    if not os.path.isfile(output.script_path):\n",
    "        raise ModelRetry(f\"The script path {output.script_path} does not exist.\")\n",
    "    return output\n",
    "\n",
    "user_prompt = f\"\"\"Your final deliverable is to cluster a dataset. The original dataset is in the directory: {os.getcwd()}/agents_workspace/data/. Take decisions on how to cluster based on the data provided.\n",
    "            You should provide a python script that takes two commnad line arguments: --input for the input dataset path and --output for the output submission file path.\n",
    "            The script should read the dataset from the input path, perform clustering, and save a submission file that contains the original columns,\n",
    "            plus a new column 'cluster_id' indicating the cluster assignment for each row.\n",
    "            Save the script as cluster.py in the current working directory ({workspace_dir}).\n",
    "            \"\"\"\n",
    "\n",
    "async def main():\n",
    "    agent_output = await run_agent( #this runs the agent loop\n",
    "        agent = agent,\n",
    "        user_prompt=user_prompt,\n",
    "        max_steps=10\n",
    "    )\n",
    "\n",
    "    evaluate_clustering(script_path=agent_output.script_path) #fixed programmatic evaluation after agent run\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "agents_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
